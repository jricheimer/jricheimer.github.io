---
layout:     post
title:      Keras Hack #1
date:       2015-03-23 15:31:19
author:     Jacob Richeimer
summary:    Adding weight decay to a Keras model
categories: jekyll
thumbnail:  heart
tags:
 - keras
---

Weight decay, or L2 regularization is a common regularization method used in training neural networks.
The idea is to add a term to the loss which signifies the magnitude of the weight values in the network,
thereby encouraging the weight values to decrease during the training process.

Intuitively, there are different ways to think about the benefit of weight decay. One way to think about it is that
only weights which consistently affect the predictions throughout the dataset will be tuned accordingly,
while the weights which are just responding to noise which has little to no effect on the loss, those weights will
be overcome by the weight decay term and will steadily decrease toward zero.

Another way to think about it is as a sort of prior. We assume that the model which can perform the task at hand
can be represented with small weights, and that the presence of overly large weights signifies an attempt to fit
to outliers or noise in the dataset - the dreaded "overfitting" scenario.

Anyhow, Keras has a built-in `Regularizer` class, and common regilarizers, like L1 and L2, can be added to each layer
independently. This means, that if you want a weight decay with coefficient `lambda` for all the weights in your network,
you need to add an instance of `regularizers.l2(lambda)` to each layer with weights (typically `Conv2D` and `Dense` layers)
as you initialize them. See the exaples in the [Keras docs][1].



[1]: https://keras.io/regularizers/
